{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f3b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "from data import DataLoader\n",
    "import importlib\n",
    "importlib.reload(data)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gpt import GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader()\n",
    "config = GPTConfig(vocab_size=50257, block_size=1024,\n",
    "                   n_layer=12, n_head=12, n_embd=768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa22075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356, 1183, 423, 11676, 379, 674, 898, 2756, 13, 198, 3792, 470, 257, 15593, 30, 198, 198, 3237, 25, 198, 2949, 517, 3375, 319, 470, 26, 1309, 340, 307, 1760, 25, 1497, 11, 1497, 0, 198, 198, 12211, 22307, 25, 198, 3198, 1573, 11, 922, 4290, 13, 198, 198, 5962, 22307, 25, 198, 1135, 389, 17830, 3595, 4290, 11, 262, 1458, 1173, 1547, 922, 13, 198, 2061, 4934, 969, 5036, 896, 319, 561, 26958, 514, 25, 611, 484, 198, 19188, 7800, 514, 475, 262, 48713, 414, 11, 981, 340, 547, 198, 1929, 4316, 462, 11, 356, 1244, 4724, 484, 22598, 514, 31533, 306, 26, 198, 4360, 484, 892, 356, 389, 1165, 13674, 25, 262, 10904, 1108, 326, 198, 2001, 42267, 514, 11, 262, 2134, 286, 674, 24672, 11, 318, 355, 281, 198, 24807, 284, 1948, 786, 511, 20038, 26, 674, 198, 82, 13712, 590, 318, 257, 4461, 284, 606, 3914, 514, 15827, 428, 351, 198, 454, 279, 7938, 11, 304, 260, 356, 1716, 374, 1124, 25, 329, 262, 11858, 760, 314, 198, 47350, 428, 287, 16460, 329, 8509, 11, 407, 287, 24613, 329, 15827, 13, 198, 198, 12211, 22307, 25, 198, 17353, 345, 5120, 2592, 1028, 327, 1872, 385, 1526, 28599, 30, 198, 198, 3237, 25, 198, 39276, 683, 717, 25, 339, 338, 257, 845, 3290, 284, 262, 2219, 6017, 13, 198, 198, 12211, 22307, 25, 198, 19626, 345, 644, 2594, 339, 468, 1760, 329, 465, 1499, 30, 198, 198, 5962, 22307, 25, 198, 16371, 880, 26, 290, 714, 307, 2695, 284, 1577, 683, 922, 198, 13116, 6285, 11, 475, 326, 339, 13831, 2241, 351, 852, 6613, 13, 198, 198, 12211, 22307, 25, 198, 45, 323, 11, 475, 2740, 407, 17412, 306, 13, 198, 198, 5962, 22307, 25, 198, 40, 910, 12722, 345, 11, 644, 339, 22027, 1760, 20524, 11, 339, 750, 198, 270, 284, 326, 886, 25, 996, 2705, 12, 5936, 979, 5864, 1450, 460, 307, 198, 11299, 284, 910, 340, 373, 329, 465, 1499, 339, 750, 340, 284, 198, 29688, 465, 2802, 290, 284, 307, 11476, 6613, 26, 543, 339, 198, 271, 11, 772, 10597, 262, 20334, 286, 465, 14675, 13, 198, 198, 12211, 22307, 25, 198, 2061, 339, 2314, 1037, 287, 465, 3450, 11, 345, 1848, 257, 198, 28281, 287, 683, 13, 921, 1276, 287, 645, 835, 910, 339, 318, 25746, 83, 516, 13, 198, 198, 5962, 22307, 25, 198, 1532, 314, 1276, 407, 11, 314, 761, 407, 307, 39497, 286, 14227, 26, 198, 258, 22027, 31025, 11, 351, 18201, 11, 284, 15867, 287, 29693, 13, 198, 2061, 34757, 389, 777, 30, 383, 584, 1735, 267, 6, 262, 1748, 198, 271, 17450, 25, 1521, 2652, 356, 778, 803, 994, 30, 284, 262, 13241, 0, 198, 198, 3237, 25, 198, 16773, 11, 1282, 13, 198, 198, 5962, 22307, 25, 198, 18380, 0, 508, 2058, 994, 30, 198, 198, 12211, 22307, 25, 198, 54, 18906, 6065, 268, 3754, 2449, 14602, 64, 26, 530, 326, 22027, 1464, 6151, 198, 1169, 661, 13, 198, 198, 5962, 22307, 25, 198, 1544, 338, 530, 5508, 1576, 25, 561, 477, 262, 1334, 547, 523, 0, 198, 198, 49275, 1677, 40, 2937, 25, 198, 2061, 670, 338, 11, 616, 1499, 3653, 11, 287, 1021, 30, 810, 467, 345, 198, 3152, 19553, 290, 9784, 30, 383, 2300, 30, 2740, 11, 314, 12472, 345, 13, 198, 198, 5962, 22307, 25, 198, 5122, 1597, 318, 407, 6439, 284, 262, 34548, 26, 484, 423, 198, 18108, 16882, 1359, 428, 46327, 644, 356, 14765, 284, 466, 11, 198, 4758, 783, 356, 1183, 905, 705, 368, 287, 23777, 13, 1119, 910, 3595, 198, 6063, 669, 423, 1913, 45576, 25, 484, 2236, 760, 356, 198, 14150, 1913, 5101, 1165, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 5195, 11, 18159, 11, 616, 922, 2460, 11, 6164, 5508, 23788, 11, 198, 8743, 345, 23981, 27012, 30, 198, 198, 5962, 22307, 25, 198, 1135, 2314, 11, 15967, 11, 356, 389, 45171, 1541, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 40, 1560, 345, 11, 2460, 11, 749, 21803, 1337, 198, 11980, 262, 1458, 1173, 1547, 286, 345, 13, 1114, 534, 3382, 11, 198, 7120, 7195, 287, 428, 390, 11999, 11, 345, 743, 355, 880, 198, 31584, 379, 262, 9538, 351, 534, 336, 3080, 355, 10303, 606, 198, 39276, 262, 7993, 1181, 11, 3025, 1781, 481, 319, 198, 464, 835, 340, 2753, 11, 25407, 3478, 7319, 1090, 1443, 198, 5189, 517, 1913, 2792, 355, 4625, 621, 460, 1683, 198, 4677, 451, 287, 534, 26795, 3681, 13, 1114, 262, 390, 11999, 11, 198, 464, 11858, 11, 407, 262, 1458, 1173, 1547, 11, 787, 340, 11, 290, 198, 7120, 14475, 284, 606, 11, 407, 5101, 11, 1276, 1037, 13, 978, 441, 11, 198, 1639, 389, 18665, 416, 35765, 414, 198, 817, 1555, 810, 517, 32743, 345, 11, 290, 345, 47397, 198, 464, 932, 907, 267, 6, 262, 1181, 11, 508, 1337, 329, 345, 588, 17150, 11, 198, 2215, 345, 17328, 606, 355, 5775, 13, 198, 198, 5962, 22307, 25, 198, 17784, 329, 514, 0, 6407, 11, 5600, 0, 1119, 497, 6, 263, 19951, 329, 514, 198, 25907, 25, 8659, 514, 284, 1145, 680, 11, 290, 511, 3650, 12, 20089, 198, 66, 859, 1150, 351, 13020, 26, 787, 1225, 14137, 329, 514, 1601, 11, 284, 198, 11284, 514, 17496, 26, 14634, 4445, 597, 17950, 462, 719, 198, 27718, 1028, 262, 5527, 11, 290, 2148, 517, 198, 79, 959, 2259, 24895, 4445, 11, 284, 6333, 510, 290, 39300, 198, 1169, 3595, 13, 1002, 262, 9976, 4483, 514, 407, 510, 11, 484, 481, 26, 290, 198, 8117, 338, 477, 262, 1842, 484, 6842, 514]\n",
      "[22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356, 1183, 423, 11676, 379, 674, 898, 2756, 13, 198, 3792, 470, 257, 15593, 30, 198, 198, 3237, 25, 198, 2949, 517, 3375, 319, 470, 26, 1309, 340, 307, 1760, 25, 1497, 11, 1497, 0, 198, 198, 12211, 22307, 25, 198, 3198, 1573, 11, 922, 4290, 13, 198, 198, 5962, 22307, 25, 198, 1135, 389, 17830, 3595, 4290, 11, 262, 1458, 1173, 1547, 922, 13, 198, 2061, 4934, 969, 5036, 896, 319, 561, 26958, 514, 25, 611, 484, 198, 19188, 7800, 514, 475, 262, 48713, 414, 11, 981, 340, 547, 198, 1929, 4316, 462, 11, 356, 1244, 4724, 484, 22598, 514, 31533, 306, 26, 198, 4360, 484, 892, 356, 389, 1165, 13674, 25, 262, 10904, 1108, 326, 198, 2001, 42267, 514, 11, 262, 2134, 286, 674, 24672, 11, 318, 355, 281, 198, 24807, 284, 1948, 786, 511, 20038, 26, 674, 198, 82, 13712, 590, 318, 257, 4461, 284, 606, 3914, 514, 15827, 428, 351, 198, 454, 279, 7938, 11, 304, 260, 356, 1716, 374, 1124, 25, 329, 262, 11858, 760, 314, 198, 47350, 428, 287, 16460, 329, 8509, 11, 407, 287, 24613, 329, 15827, 13, 198, 198, 12211, 22307, 25, 198, 17353, 345, 5120, 2592, 1028, 327, 1872, 385, 1526, 28599, 30, 198, 198, 3237, 25, 198, 39276, 683, 717, 25, 339, 338, 257, 845, 3290, 284, 262, 2219, 6017, 13, 198, 198, 12211, 22307, 25, 198, 19626, 345, 644, 2594, 339, 468, 1760, 329, 465, 1499, 30, 198, 198, 5962, 22307, 25, 198, 16371, 880, 26, 290, 714, 307, 2695, 284, 1577, 683, 922, 198, 13116, 6285, 11, 475, 326, 339, 13831, 2241, 351, 852, 6613, 13, 198, 198, 12211, 22307, 25, 198, 45, 323, 11, 475, 2740, 407, 17412, 306, 13, 198, 198, 5962, 22307, 25, 198, 40, 910, 12722, 345, 11, 644, 339, 22027, 1760, 20524, 11, 339, 750, 198, 270, 284, 326, 886, 25, 996, 2705, 12, 5936, 979, 5864, 1450, 460, 307, 198, 11299, 284, 910, 340, 373, 329, 465, 1499, 339, 750, 340, 284, 198, 29688, 465, 2802, 290, 284, 307, 11476, 6613, 26, 543, 339, 198, 271, 11, 772, 10597, 262, 20334, 286, 465, 14675, 13, 198, 198, 12211, 22307, 25, 198, 2061, 339, 2314, 1037, 287, 465, 3450, 11, 345, 1848, 257, 198, 28281, 287, 683, 13, 921, 1276, 287, 645, 835, 910, 339, 318, 25746, 83, 516, 13, 198, 198, 5962, 22307, 25, 198, 1532, 314, 1276, 407, 11, 314, 761, 407, 307, 39497, 286, 14227, 26, 198, 258, 22027, 31025, 11, 351, 18201, 11, 284, 15867, 287, 29693, 13, 198, 2061, 34757, 389, 777, 30, 383, 584, 1735, 267, 6, 262, 1748, 198, 271, 17450, 25, 1521, 2652, 356, 778, 803, 994, 30, 284, 262, 13241, 0, 198, 198, 3237, 25, 198, 16773, 11, 1282, 13, 198, 198, 5962, 22307, 25, 198, 18380, 0, 508, 2058, 994, 30, 198, 198, 12211, 22307, 25, 198, 54, 18906, 6065, 268, 3754, 2449, 14602, 64, 26, 530, 326, 22027, 1464, 6151, 198, 1169, 661, 13, 198, 198, 5962, 22307, 25, 198, 1544, 338, 530, 5508, 1576, 25, 561, 477, 262, 1334, 547, 523, 0, 198, 198, 49275, 1677, 40, 2937, 25, 198, 2061, 670, 338, 11, 616, 1499, 3653, 11, 287, 1021, 30, 810, 467, 345, 198, 3152, 19553, 290, 9784, 30, 383, 2300, 30, 2740, 11, 314, 12472, 345, 13, 198, 198, 5962, 22307, 25, 198, 5122, 1597, 318, 407, 6439, 284, 262, 34548, 26, 484, 423, 198, 18108, 16882, 1359, 428, 46327, 644, 356, 14765, 284, 466, 11, 198, 4758, 783, 356, 1183, 905, 705, 368, 287, 23777, 13, 1119, 910, 3595, 198, 6063, 669, 423, 1913, 45576, 25, 484, 2236, 760, 356, 198, 14150, 1913, 5101, 1165, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 5195, 11, 18159, 11, 616, 922, 2460, 11, 6164, 5508, 23788, 11, 198, 8743, 345, 23981, 27012, 30, 198, 198, 5962, 22307, 25, 198, 1135, 2314, 11, 15967, 11, 356, 389, 45171, 1541, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 40, 1560, 345, 11, 2460, 11, 749, 21803, 1337, 198, 11980, 262, 1458, 1173, 1547, 286, 345, 13, 1114, 534, 3382, 11, 198, 7120, 7195, 287, 428, 390, 11999, 11, 345, 743, 355, 880, 198, 31584, 379, 262, 9538, 351, 534, 336, 3080, 355, 10303, 606, 198, 39276, 262, 7993, 1181, 11, 3025, 1781, 481, 319, 198, 464, 835, 340, 2753, 11, 25407, 3478, 7319, 1090, 1443, 198, 5189, 517, 1913, 2792, 355, 4625, 621, 460, 1683, 198, 4677, 451, 287, 534, 26795, 3681, 13, 1114, 262, 390, 11999, 11, 198, 464, 11858, 11, 407, 262, 1458, 1173, 1547, 11, 787, 340, 11, 290, 198, 7120, 14475, 284, 606, 11, 407, 5101, 11, 1276, 1037, 13, 978, 441, 11, 198, 1639, 389, 18665, 416, 35765, 414, 198, 817, 1555, 810, 517, 32743, 345, 11, 290, 345, 47397, 198, 464, 932, 907, 267, 6, 262, 1181, 11, 508, 1337, 329, 345, 588, 17150, 11, 198, 2215, 345, 17328, 606, 355, 5775, 13, 198, 198, 5962, 22307, 25, 198, 17784, 329, 514, 0, 6407, 11, 5600, 0, 1119, 497, 6, 263, 19951, 329, 514, 198, 25907, 25, 8659, 514, 284, 1145, 680, 11, 290, 511, 3650, 12, 20089, 198, 66, 859, 1150, 351, 13020, 26, 787, 1225, 14137, 329, 514, 1601, 11, 284, 198, 11284, 514, 17496, 26, 14634, 4445, 597, 17950, 462, 719, 198, 27718, 1028, 262, 5527, 11, 290, 2148, 517, 198, 79, 959, 2259, 24895, 4445, 11, 284, 6333, 510, 290, 39300, 198, 1169, 3595, 13, 1002, 262, 9976, 4483, 514, 407, 510, 11, 484, 481, 26, 290, 198, 8117, 338, 477, 262, 1842, 484, 6842, 514, 13]\n"
     ]
    }
   ],
   "source": [
    "print(data.inputs[0])\n",
    "print(data.outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4bcf15a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[7., 4., 1., 7.],\n",
      "        [5., 8., 4., 0.],\n",
      "        [0., 6., 3., 1.],\n",
      "        [8., 4., 4., 0.]])\n",
      "tensor([[7., -inf, -inf, -inf],\n",
      "        [5., 8., -inf, -inf],\n",
      "        [0., 6., 3., -inf],\n",
      "        [8., 4., 4., 0.]])\n"
     ]
    }
   ],
   "source": [
    "one_tri = torch.tril(torch.ones(4,4))\n",
    "print(one_tri)\n",
    "m = torch.randint(0, 10, (4,4)).float()\n",
    "print(m)\n",
    "\n",
    "masked = m.masked_fill(one_tri==0, float('-inf'))\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c34c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454bdc84577b48d5ad2c756ff25953a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df54fa767dd4675abf6e4da827bfc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0554ce13bf834f38b8b3545fc83e142d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf45214b75494d87ba4c9fe194c81c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qadata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8d8397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: databricks/databricks-dolly-15k\n",
      "train dataset size: 13487 examples\n",
      "val dataset size: 1501 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - Input: torch.Size([8, 255]), Target: torch.Size([8, 255])\n",
      "Vocabulary size: 32000\n",
      "\n",
      "Sample input (truncated):\n",
      "<s> List 3 cities on the east coast of Australia. Sydney, Brisbane and Newcastle are Australian cities located on the east coast of Australia.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></...\n",
      "\n",
      "\n",
      "Modern training loop example:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Input shape torch.Size([8, 255]), Target shape torch.Size([8, 255])\n",
      "tensor([    1,  3529,  2367,   592,   263, 11473,  4955,   310,   278, 25200,\n",
      "         8068,  9538, 29973, 20628,   727,   526, 29871, 29906, 29947,   970,\n",
      "         9562,   297, 25200,   541, 29892,   408,   278,   937,   304,  1722,\n",
      "          297,   278, 17780,  7483, 29892,   278, 11265,   310,  8068,  9538,\n",
      "          471,  5220,   287,   411, 15151, 29945, 29900, 29892, 29900, 29900,\n",
      "        29900,   491,  8578,  9716,  1336,   391, 11571,  1704, 10052,   347,\n",
      "        29889,  2180,   278,  8718, 25672,   263,  4382,  1393,   515,  1704,\n",
      "        10052,   347,   471,  1303,   714, 23659, 29901,   376,  4806,  9311,\n",
      "          393,   445,  9538,   338,   304,  6548,   297,  5407,  2264,  1629,\n",
      "         1156,  1629, 29892,   322,  6356,   697,   310,   278,  1556,  3104,\n",
      "          296,   946, 15942,   363,   278,  1781,   310,   278,  2305,   363,\n",
      "          599,   931,   304,  2041,  1213,    13,    13,  1576,  3268,  4629,\n",
      "          363,   278,  3489,   471,   278,  4642,  3271,   310,  6290,  5569,\n",
      "         7963, 29892, 29871, 29896,   303, 15991,   300,  7963,   310, 14279,\n",
      "         1141,   497, 29892, 22545,   403,   363,  4088,  5322,   306, 29889,\n",
      "          450,  3829, 29892,  4240,   297, 29871, 29896, 29953, 29896, 29953,\n",
      "        29892,   471, 27745,  3276,   297,  4779, 29871, 29896, 29947, 29947,\n",
      "        29955,   304,  1207,   982,   363,   278,  3489, 29889,   450,   301,\n",
      "          524,   295,   515,  7963, 29915, 29879,  3271, 29892, 24638,   278,\n",
      "         1559,  1490,  1663,  3395,   323, 11206,  5005,   379,  2882,  1806,\n",
      "        29909, 29871, 29896, 29953, 29896, 29953,   515,   278, 11582,  3290,\n",
      "          533,   310,  9034,  2482, 29892,   338, 21634,  2038,   385,  6426,\n",
      "         3050,  1582,   310,   278,  3489, 29889,    13,    13, 29907,  2753,\n",
      "          387,   347, 29915, 29879,  5220,   292,   471, 12919,   385,  5957,\n",
      "          310, 15151, 29906, 29945, 29892, 29900, 29900, 29900,   297, 29871,\n",
      "        29896, 29947, 29947, 29953,   607], device='cuda:0') tensor([ 3529,  2367,   592,   263, 11473,  4955,   310,   278, 25200,  8068,\n",
      "         9538, 29973, 20628,   727,   526, 29871, 29906, 29947,   970,  9562,\n",
      "          297, 25200,   541, 29892,   408,   278,   937,   304,  1722,   297,\n",
      "          278, 17780,  7483, 29892,   278, 11265,   310,  8068,  9538,   471,\n",
      "         5220,   287,   411, 15151, 29945, 29900, 29892, 29900, 29900, 29900,\n",
      "          491,  8578,  9716,  1336,   391, 11571,  1704, 10052,   347, 29889,\n",
      "         2180,   278,  8718, 25672,   263,  4382,  1393,   515,  1704, 10052,\n",
      "          347,   471,  1303,   714, 23659, 29901,   376,  4806,  9311,   393,\n",
      "          445,  9538,   338,   304,  6548,   297,  5407,  2264,  1629,  1156,\n",
      "         1629, 29892,   322,  6356,   697,   310,   278,  1556,  3104,   296,\n",
      "          946, 15942,   363,   278,  1781,   310,   278,  2305,   363,   599,\n",
      "          931,   304,  2041,  1213,    13,    13,  1576,  3268,  4629,   363,\n",
      "          278,  3489,   471,   278,  4642,  3271,   310,  6290,  5569,  7963,\n",
      "        29892, 29871, 29896,   303, 15991,   300,  7963,   310, 14279,  1141,\n",
      "          497, 29892, 22545,   403,   363,  4088,  5322,   306, 29889,   450,\n",
      "         3829, 29892,  4240,   297, 29871, 29896, 29953, 29896, 29953, 29892,\n",
      "          471, 27745,  3276,   297,  4779, 29871, 29896, 29947, 29947, 29955,\n",
      "          304,  1207,   982,   363,   278,  3489, 29889,   450,   301,   524,\n",
      "          295,   515,  7963, 29915, 29879,  3271, 29892, 24638,   278,  1559,\n",
      "         1490,  1663,  3395,   323, 11206,  5005,   379,  2882,  1806, 29909,\n",
      "        29871, 29896, 29953, 29896, 29953,   515,   278, 11582,  3290,   533,\n",
      "          310,  9034,  2482, 29892,   338, 21634,  2038,   385,  6426,  3050,\n",
      "         1582,   310,   278,  3489, 29889,    13,    13, 29907,  2753,   387,\n",
      "          347, 29915, 29879,  5220,   292,   471, 12919,   385,  5957,   310,\n",
      "        15151, 29906, 29945, 29892, 29900, 29900, 29900,   297, 29871, 29896,\n",
      "        29947, 29947, 29953,   607,   471], device='cuda:0')\n",
      "Batch 1: Input shape torch.Size([8, 255]), Target shape torch.Size([8, 255])\n",
      "tensor([    1,  8561,   263,  1051, 29892,   773,  8329,  3291, 29892,   310,\n",
      "          278,  1833,  5320, 14368,  1058, 17791,   278, 16373, 29889,   450,\n",
      "         1833,  5320, 14368,   304,  3495,   278, 19025, 12482,   892, 29901,\n",
      "           13, 29899, 20377, 29892,  5546,   313, 29906, 29900, 29906, 29896,\n",
      "          511,    13, 29899, 11611, 29892, 16078,   313, 29906, 29900, 29896,\n",
      "        29953,   511,    13, 29899,  4517, 29892,  5408,   313, 29906, 29900,\n",
      "        29896, 29906,   511,    13, 29899,  1522,   823,   292, 29892,  7551,\n",
      "          313, 29906, 29900, 29900, 29947,   511,    13, 29899,  9193,   575,\n",
      "        29892, 25549,   313, 29906, 29900, 29900, 29946,   467,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2], device='cuda:0') tensor([ 8561,   263,  1051, 29892,   773,  8329,  3291, 29892,   310,   278,\n",
      "         1833,  5320, 14368,  1058, 17791,   278, 16373, 29889,   450,  1833,\n",
      "         5320, 14368,   304,  3495,   278, 19025, 12482,   892, 29901,    13,\n",
      "        29899, 20377, 29892,  5546,   313, 29906, 29900, 29906, 29896,   511,\n",
      "           13, 29899, 11611, 29892, 16078,   313, 29906, 29900, 29896, 29953,\n",
      "          511,    13, 29899,  4517, 29892,  5408,   313, 29906, 29900, 29896,\n",
      "        29906,   511,    13, 29899,  1522,   823,   292, 29892,  7551,   313,\n",
      "        29906, 29900, 29900, 29947,   511,    13, 29899,  9193,   575, 29892,\n",
      "        25549,   313, 29906, 29900, 29900, 29946,   467,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "qa_loader = QADataLoader(\n",
    "    dataset_name=\"databricks/databricks-dolly-15k\",\n",
    "    batch_size=batch_size,\n",
    "    block_size=block_size\n",
    ")\n",
    "\n",
    "# Test getting a batch (old style)\n",
    "x, y = qa_loader.get_batch('train')\n",
    "print(f\"Batch shapes - Input: {x.shape}, Target: {y.shape}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Decode and show a sample\n",
    "sample_input = tokenizer.decode(x[0].tolist())\n",
    "print(f\"\\nSample input (truncated):\\n{sample_input[:200]}...\")\n",
    "\n",
    "# Modern training loop example\n",
    "print(\"\\n\\nModern training loop example:\")\n",
    "train_loader, val_loader = qa_loader.get_loaders()\n",
    "\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    print(f\"Batch {i}: Input shape {inputs.shape}, Target shape {targets.shape}\")\n",
    "    print(inputs[0], targets[0])\n",
    "    \n",
    "    if i >= 1:  # Just show first 3 batches\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
