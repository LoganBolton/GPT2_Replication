{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f3b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "from data import DataLoader\n",
    "import importlib\n",
    "importlib.reload(data)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gpt import GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader()\n",
    "config = GPTConfig(vocab_size=50257, block_size=1024,\n",
    "                   n_layer=12, n_head=12, n_embd=768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa22075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356, 1183, 423, 11676, 379, 674, 898, 2756, 13, 198, 3792, 470, 257, 15593, 30, 198, 198, 3237, 25, 198, 2949, 517, 3375, 319, 470, 26, 1309, 340, 307, 1760, 25, 1497, 11, 1497, 0, 198, 198, 12211, 22307, 25, 198, 3198, 1573, 11, 922, 4290, 13, 198, 198, 5962, 22307, 25, 198, 1135, 389, 17830, 3595, 4290, 11, 262, 1458, 1173, 1547, 922, 13, 198, 2061, 4934, 969, 5036, 896, 319, 561, 26958, 514, 25, 611, 484, 198, 19188, 7800, 514, 475, 262, 48713, 414, 11, 981, 340, 547, 198, 1929, 4316, 462, 11, 356, 1244, 4724, 484, 22598, 514, 31533, 306, 26, 198, 4360, 484, 892, 356, 389, 1165, 13674, 25, 262, 10904, 1108, 326, 198, 2001, 42267, 514, 11, 262, 2134, 286, 674, 24672, 11, 318, 355, 281, 198, 24807, 284, 1948, 786, 511, 20038, 26, 674, 198, 82, 13712, 590, 318, 257, 4461, 284, 606, 3914, 514, 15827, 428, 351, 198, 454, 279, 7938, 11, 304, 260, 356, 1716, 374, 1124, 25, 329, 262, 11858, 760, 314, 198, 47350, 428, 287, 16460, 329, 8509, 11, 407, 287, 24613, 329, 15827, 13, 198, 198, 12211, 22307, 25, 198, 17353, 345, 5120, 2592, 1028, 327, 1872, 385, 1526, 28599, 30, 198, 198, 3237, 25, 198, 39276, 683, 717, 25, 339, 338, 257, 845, 3290, 284, 262, 2219, 6017, 13, 198, 198, 12211, 22307, 25, 198, 19626, 345, 644, 2594, 339, 468, 1760, 329, 465, 1499, 30, 198, 198, 5962, 22307, 25, 198, 16371, 880, 26, 290, 714, 307, 2695, 284, 1577, 683, 922, 198, 13116, 6285, 11, 475, 326, 339, 13831, 2241, 351, 852, 6613, 13, 198, 198, 12211, 22307, 25, 198, 45, 323, 11, 475, 2740, 407, 17412, 306, 13, 198, 198, 5962, 22307, 25, 198, 40, 910, 12722, 345, 11, 644, 339, 22027, 1760, 20524, 11, 339, 750, 198, 270, 284, 326, 886, 25, 996, 2705, 12, 5936, 979, 5864, 1450, 460, 307, 198, 11299, 284, 910, 340, 373, 329, 465, 1499, 339, 750, 340, 284, 198, 29688, 465, 2802, 290, 284, 307, 11476, 6613, 26, 543, 339, 198, 271, 11, 772, 10597, 262, 20334, 286, 465, 14675, 13, 198, 198, 12211, 22307, 25, 198, 2061, 339, 2314, 1037, 287, 465, 3450, 11, 345, 1848, 257, 198, 28281, 287, 683, 13, 921, 1276, 287, 645, 835, 910, 339, 318, 25746, 83, 516, 13, 198, 198, 5962, 22307, 25, 198, 1532, 314, 1276, 407, 11, 314, 761, 407, 307, 39497, 286, 14227, 26, 198, 258, 22027, 31025, 11, 351, 18201, 11, 284, 15867, 287, 29693, 13, 198, 2061, 34757, 389, 777, 30, 383, 584, 1735, 267, 6, 262, 1748, 198, 271, 17450, 25, 1521, 2652, 356, 778, 803, 994, 30, 284, 262, 13241, 0, 198, 198, 3237, 25, 198, 16773, 11, 1282, 13, 198, 198, 5962, 22307, 25, 198, 18380, 0, 508, 2058, 994, 30, 198, 198, 12211, 22307, 25, 198, 54, 18906, 6065, 268, 3754, 2449, 14602, 64, 26, 530, 326, 22027, 1464, 6151, 198, 1169, 661, 13, 198, 198, 5962, 22307, 25, 198, 1544, 338, 530, 5508, 1576, 25, 561, 477, 262, 1334, 547, 523, 0, 198, 198, 49275, 1677, 40, 2937, 25, 198, 2061, 670, 338, 11, 616, 1499, 3653, 11, 287, 1021, 30, 810, 467, 345, 198, 3152, 19553, 290, 9784, 30, 383, 2300, 30, 2740, 11, 314, 12472, 345, 13, 198, 198, 5962, 22307, 25, 198, 5122, 1597, 318, 407, 6439, 284, 262, 34548, 26, 484, 423, 198, 18108, 16882, 1359, 428, 46327, 644, 356, 14765, 284, 466, 11, 198, 4758, 783, 356, 1183, 905, 705, 368, 287, 23777, 13, 1119, 910, 3595, 198, 6063, 669, 423, 1913, 45576, 25, 484, 2236, 760, 356, 198, 14150, 1913, 5101, 1165, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 5195, 11, 18159, 11, 616, 922, 2460, 11, 6164, 5508, 23788, 11, 198, 8743, 345, 23981, 27012, 30, 198, 198, 5962, 22307, 25, 198, 1135, 2314, 11, 15967, 11, 356, 389, 45171, 1541, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 40, 1560, 345, 11, 2460, 11, 749, 21803, 1337, 198, 11980, 262, 1458, 1173, 1547, 286, 345, 13, 1114, 534, 3382, 11, 198, 7120, 7195, 287, 428, 390, 11999, 11, 345, 743, 355, 880, 198, 31584, 379, 262, 9538, 351, 534, 336, 3080, 355, 10303, 606, 198, 39276, 262, 7993, 1181, 11, 3025, 1781, 481, 319, 198, 464, 835, 340, 2753, 11, 25407, 3478, 7319, 1090, 1443, 198, 5189, 517, 1913, 2792, 355, 4625, 621, 460, 1683, 198, 4677, 451, 287, 534, 26795, 3681, 13, 1114, 262, 390, 11999, 11, 198, 464, 11858, 11, 407, 262, 1458, 1173, 1547, 11, 787, 340, 11, 290, 198, 7120, 14475, 284, 606, 11, 407, 5101, 11, 1276, 1037, 13, 978, 441, 11, 198, 1639, 389, 18665, 416, 35765, 414, 198, 817, 1555, 810, 517, 32743, 345, 11, 290, 345, 47397, 198, 464, 932, 907, 267, 6, 262, 1181, 11, 508, 1337, 329, 345, 588, 17150, 11, 198, 2215, 345, 17328, 606, 355, 5775, 13, 198, 198, 5962, 22307, 25, 198, 17784, 329, 514, 0, 6407, 11, 5600, 0, 1119, 497, 6, 263, 19951, 329, 514, 198, 25907, 25, 8659, 514, 284, 1145, 680, 11, 290, 511, 3650, 12, 20089, 198, 66, 859, 1150, 351, 13020, 26, 787, 1225, 14137, 329, 514, 1601, 11, 284, 198, 11284, 514, 17496, 26, 14634, 4445, 597, 17950, 462, 719, 198, 27718, 1028, 262, 5527, 11, 290, 2148, 517, 198, 79, 959, 2259, 24895, 4445, 11, 284, 6333, 510, 290, 39300, 198, 1169, 3595, 13, 1002, 262, 9976, 4483, 514, 407, 510, 11, 484, 481, 26, 290, 198, 8117, 338, 477, 262, 1842, 484, 6842, 514]\n",
      "[22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356, 1183, 423, 11676, 379, 674, 898, 2756, 13, 198, 3792, 470, 257, 15593, 30, 198, 198, 3237, 25, 198, 2949, 517, 3375, 319, 470, 26, 1309, 340, 307, 1760, 25, 1497, 11, 1497, 0, 198, 198, 12211, 22307, 25, 198, 3198, 1573, 11, 922, 4290, 13, 198, 198, 5962, 22307, 25, 198, 1135, 389, 17830, 3595, 4290, 11, 262, 1458, 1173, 1547, 922, 13, 198, 2061, 4934, 969, 5036, 896, 319, 561, 26958, 514, 25, 611, 484, 198, 19188, 7800, 514, 475, 262, 48713, 414, 11, 981, 340, 547, 198, 1929, 4316, 462, 11, 356, 1244, 4724, 484, 22598, 514, 31533, 306, 26, 198, 4360, 484, 892, 356, 389, 1165, 13674, 25, 262, 10904, 1108, 326, 198, 2001, 42267, 514, 11, 262, 2134, 286, 674, 24672, 11, 318, 355, 281, 198, 24807, 284, 1948, 786, 511, 20038, 26, 674, 198, 82, 13712, 590, 318, 257, 4461, 284, 606, 3914, 514, 15827, 428, 351, 198, 454, 279, 7938, 11, 304, 260, 356, 1716, 374, 1124, 25, 329, 262, 11858, 760, 314, 198, 47350, 428, 287, 16460, 329, 8509, 11, 407, 287, 24613, 329, 15827, 13, 198, 198, 12211, 22307, 25, 198, 17353, 345, 5120, 2592, 1028, 327, 1872, 385, 1526, 28599, 30, 198, 198, 3237, 25, 198, 39276, 683, 717, 25, 339, 338, 257, 845, 3290, 284, 262, 2219, 6017, 13, 198, 198, 12211, 22307, 25, 198, 19626, 345, 644, 2594, 339, 468, 1760, 329, 465, 1499, 30, 198, 198, 5962, 22307, 25, 198, 16371, 880, 26, 290, 714, 307, 2695, 284, 1577, 683, 922, 198, 13116, 6285, 11, 475, 326, 339, 13831, 2241, 351, 852, 6613, 13, 198, 198, 12211, 22307, 25, 198, 45, 323, 11, 475, 2740, 407, 17412, 306, 13, 198, 198, 5962, 22307, 25, 198, 40, 910, 12722, 345, 11, 644, 339, 22027, 1760, 20524, 11, 339, 750, 198, 270, 284, 326, 886, 25, 996, 2705, 12, 5936, 979, 5864, 1450, 460, 307, 198, 11299, 284, 910, 340, 373, 329, 465, 1499, 339, 750, 340, 284, 198, 29688, 465, 2802, 290, 284, 307, 11476, 6613, 26, 543, 339, 198, 271, 11, 772, 10597, 262, 20334, 286, 465, 14675, 13, 198, 198, 12211, 22307, 25, 198, 2061, 339, 2314, 1037, 287, 465, 3450, 11, 345, 1848, 257, 198, 28281, 287, 683, 13, 921, 1276, 287, 645, 835, 910, 339, 318, 25746, 83, 516, 13, 198, 198, 5962, 22307, 25, 198, 1532, 314, 1276, 407, 11, 314, 761, 407, 307, 39497, 286, 14227, 26, 198, 258, 22027, 31025, 11, 351, 18201, 11, 284, 15867, 287, 29693, 13, 198, 2061, 34757, 389, 777, 30, 383, 584, 1735, 267, 6, 262, 1748, 198, 271, 17450, 25, 1521, 2652, 356, 778, 803, 994, 30, 284, 262, 13241, 0, 198, 198, 3237, 25, 198, 16773, 11, 1282, 13, 198, 198, 5962, 22307, 25, 198, 18380, 0, 508, 2058, 994, 30, 198, 198, 12211, 22307, 25, 198, 54, 18906, 6065, 268, 3754, 2449, 14602, 64, 26, 530, 326, 22027, 1464, 6151, 198, 1169, 661, 13, 198, 198, 5962, 22307, 25, 198, 1544, 338, 530, 5508, 1576, 25, 561, 477, 262, 1334, 547, 523, 0, 198, 198, 49275, 1677, 40, 2937, 25, 198, 2061, 670, 338, 11, 616, 1499, 3653, 11, 287, 1021, 30, 810, 467, 345, 198, 3152, 19553, 290, 9784, 30, 383, 2300, 30, 2740, 11, 314, 12472, 345, 13, 198, 198, 5962, 22307, 25, 198, 5122, 1597, 318, 407, 6439, 284, 262, 34548, 26, 484, 423, 198, 18108, 16882, 1359, 428, 46327, 644, 356, 14765, 284, 466, 11, 198, 4758, 783, 356, 1183, 905, 705, 368, 287, 23777, 13, 1119, 910, 3595, 198, 6063, 669, 423, 1913, 45576, 25, 484, 2236, 760, 356, 198, 14150, 1913, 5101, 1165, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 5195, 11, 18159, 11, 616, 922, 2460, 11, 6164, 5508, 23788, 11, 198, 8743, 345, 23981, 27012, 30, 198, 198, 5962, 22307, 25, 198, 1135, 2314, 11, 15967, 11, 356, 389, 45171, 1541, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 40, 1560, 345, 11, 2460, 11, 749, 21803, 1337, 198, 11980, 262, 1458, 1173, 1547, 286, 345, 13, 1114, 534, 3382, 11, 198, 7120, 7195, 287, 428, 390, 11999, 11, 345, 743, 355, 880, 198, 31584, 379, 262, 9538, 351, 534, 336, 3080, 355, 10303, 606, 198, 39276, 262, 7993, 1181, 11, 3025, 1781, 481, 319, 198, 464, 835, 340, 2753, 11, 25407, 3478, 7319, 1090, 1443, 198, 5189, 517, 1913, 2792, 355, 4625, 621, 460, 1683, 198, 4677, 451, 287, 534, 26795, 3681, 13, 1114, 262, 390, 11999, 11, 198, 464, 11858, 11, 407, 262, 1458, 1173, 1547, 11, 787, 340, 11, 290, 198, 7120, 14475, 284, 606, 11, 407, 5101, 11, 1276, 1037, 13, 978, 441, 11, 198, 1639, 389, 18665, 416, 35765, 414, 198, 817, 1555, 810, 517, 32743, 345, 11, 290, 345, 47397, 198, 464, 932, 907, 267, 6, 262, 1181, 11, 508, 1337, 329, 345, 588, 17150, 11, 198, 2215, 345, 17328, 606, 355, 5775, 13, 198, 198, 5962, 22307, 25, 198, 17784, 329, 514, 0, 6407, 11, 5600, 0, 1119, 497, 6, 263, 19951, 329, 514, 198, 25907, 25, 8659, 514, 284, 1145, 680, 11, 290, 511, 3650, 12, 20089, 198, 66, 859, 1150, 351, 13020, 26, 787, 1225, 14137, 329, 514, 1601, 11, 284, 198, 11284, 514, 17496, 26, 14634, 4445, 597, 17950, 462, 719, 198, 27718, 1028, 262, 5527, 11, 290, 2148, 517, 198, 79, 959, 2259, 24895, 4445, 11, 284, 6333, 510, 290, 39300, 198, 1169, 3595, 13, 1002, 262, 9976, 4483, 514, 407, 510, 11, 484, 481, 26, 290, 198, 8117, 338, 477, 262, 1842, 484, 6842, 514, 13]\n"
     ]
    }
   ],
   "source": [
    "print(data.inputs[0])\n",
    "print(data.outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4bcf15a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[7., 4., 1., 7.],\n",
      "        [5., 8., 4., 0.],\n",
      "        [0., 6., 3., 1.],\n",
      "        [8., 4., 4., 0.]])\n",
      "tensor([[7., -inf, -inf, -inf],\n",
      "        [5., 8., -inf, -inf],\n",
      "        [0., 6., 3., -inf],\n",
      "        [8., 4., 4., 0.]])\n"
     ]
    }
   ],
   "source": [
    "one_tri = torch.tril(torch.ones(4,4))\n",
    "print(one_tri)\n",
    "m = torch.randint(0, 10, (4,4)).float()\n",
    "print(m)\n",
    "\n",
    "masked = m.masked_fill(one_tri==0, float('-inf'))\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c34c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qadata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8d8397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: databricks/databricks-dolly-15k\n",
      "Batch shapes - Input: torch.Size([32, 511]), Target: torch.Size([32, 511])\n",
      "Vocabulary size: 32000\n",
      "\n",
      "Sample input (truncated):\n",
      "<s> ### USER: Classify each as either an amphibian, bird, or fish: salmon, trout, salamander, cardinal, owl, falcon, frog\n",
      "\n",
      "### Answer: Salmon: Fish\n",
      "Trout: Fish\n",
      "Salamander: Amphibian\n",
      "Cardinal: Bird\n",
      "Owl...\n",
      "\n",
      "\n",
      "Modern training loop example:\n",
      "423\n",
      "Batch 0: Input shape torch.Size([32, 511]), Target shape torch.Size([32, 511])\n",
      "tensor([    1,   835,  3148,  1001, 29901,  3529,  2367,  3273, 15837,   310,\n",
      "        15221, 29875, 18939, 29926,   350, 28690, 14113,  5073,  2729,   373,\n",
      "         2400,    13,    13,  2277, 29937,   673, 29901, 15221, 29875, 18939,\n",
      "        29926,   350, 28690, 14113,  6496,   297, 29871, 29896, 29929, 29906,\n",
      "        29947,   297, 19628,  9188,  6474, 29889,   450, 15221, 29875, 18939,\n",
      "        29926, 14113,  1196, 19623,   515, 15221, 11163, 29926,  6474, 26463,\n",
      "         4726,   304,   278,  7392,   433,  5139,  3025,  1383, 29874,   391,\n",
      "        18939, 29926,   432,   651, 29889,  3189,   284, 29899,   996,  1312,\n",
      "        22983,  1304,   304,  1065,  1546,  9475, 16355, 29901, 15221, 11163,\n",
      "        29926,   350, 28690, 29892, 15221, 11163, 29926,  9245, 29892,  1383,\n",
      "        29874,   391, 18939, 29926,   435,   651, 29892,  1383,   557,   381,\n",
      "        12929,  4850,   328, 29892,   678,   348,   279,   688,  2455, 29892,\n",
      "         1913,  2192,   328, 29892,  4007,   314, 22752, 29892,   322,  7392,\n",
      "          433,  5139,   292,   323,  6472,  2002, 29889,  2180,   393,   931,\n",
      "        29892,   445, 14113,   471,   278,   871,  2794,   310,  5609,   292,\n",
      "        23429, 11308,   515,   278, 29871, 29896, 29941, 17161,   575,   297,\n",
      "          678,   348,   279,   688,  2455,   701,   834,  4233,   310, 15221,\n",
      "        11163, 29926,   472,   263,  4482,  3438, 29889,    13, 13555,   278,\n",
      "        21820,   310, 14320, 29880, 21754, 29892,   278, 13500,   310,   278,\n",
      "         7392,   433,  7945, 11664, 29889,   450,  7945, 29915, 29879,  6297,\n",
      "          472,   278,   931,   471,   304,  6963, 25447,   267,  1250,   515,\n",
      "         7513,   411,   385,  2715,  2977,   295,  6012, 29889,   450,  7945,\n",
      "         1304,   304,  9850,  8951,   263,  2462,   515, 15221, 11163, 29926,\n",
      "          304,   278,  5139,  5073,   472,  7392,   433, 29889,  2860,   278,\n",
      "         1095,   310,   278, 25447, 29872,  8608,   362,  8576, 29892,   278,\n",
      "         7945,   310,   350,  1964,  3289,  3897,   278,  7945,   310,  1560,\n",
      "          688,  3820,   414, 29889, 17250,   368, 29892, 28134, 10021,   287,\n",
      "        29892,   541,   278,  7945, 17654, 28495,   408,   896,  1033,  9850,\n",
      "         1728,   263, 23381, 29889,  7311,   310, 28495, 29892,  7945, 21142,\n",
      "         8872,  2760,   278, 25615,   362,   664, 29889,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2], device='cuda:0') tensor([  835,  3148,  1001, 29901,  3529,  2367,  3273, 15837,   310, 15221,\n",
      "        29875, 18939, 29926,   350, 28690, 14113,  5073,  2729,   373,  2400,\n",
      "           13,    13,  2277, 29937,   673, 29901, 15221, 29875, 18939, 29926,\n",
      "          350, 28690, 14113,  6496,   297, 29871, 29896, 29929, 29906, 29947,\n",
      "          297, 19628,  9188,  6474, 29889,   450, 15221, 29875, 18939, 29926,\n",
      "        14113,  1196, 19623,   515, 15221, 11163, 29926,  6474, 26463,  4726,\n",
      "          304,   278,  7392,   433,  5139,  3025,  1383, 29874,   391, 18939,\n",
      "        29926,   432,   651, 29889,  3189,   284, 29899,   996,  1312, 22983,\n",
      "         1304,   304,  1065,  1546,  9475, 16355, 29901, 15221, 11163, 29926,\n",
      "          350, 28690, 29892, 15221, 11163, 29926,  9245, 29892,  1383, 29874,\n",
      "          391, 18939, 29926,   435,   651, 29892,  1383,   557,   381, 12929,\n",
      "         4850,   328, 29892,   678,   348,   279,   688,  2455, 29892,  1913,\n",
      "         2192,   328, 29892,  4007,   314, 22752, 29892,   322,  7392,   433,\n",
      "         5139,   292,   323,  6472,  2002, 29889,  2180,   393,   931, 29892,\n",
      "          445, 14113,   471,   278,   871,  2794,   310,  5609,   292, 23429,\n",
      "        11308,   515,   278, 29871, 29896, 29941, 17161,   575,   297,   678,\n",
      "          348,   279,   688,  2455,   701,   834,  4233,   310, 15221, 11163,\n",
      "        29926,   472,   263,  4482,  3438, 29889,    13, 13555,   278, 21820,\n",
      "          310, 14320, 29880, 21754, 29892,   278, 13500,   310,   278,  7392,\n",
      "          433,  7945, 11664, 29889,   450,  7945, 29915, 29879,  6297,   472,\n",
      "          278,   931,   471,   304,  6963, 25447,   267,  1250,   515,  7513,\n",
      "          411,   385,  2715,  2977,   295,  6012, 29889,   450,  7945,  1304,\n",
      "          304,  9850,  8951,   263,  2462,   515, 15221, 11163, 29926,   304,\n",
      "          278,  5139,  5073,   472,  7392,   433, 29889,  2860,   278,  1095,\n",
      "          310,   278, 25447, 29872,  8608,   362,  8576, 29892,   278,  7945,\n",
      "          310,   350,  1964,  3289,  3897,   278,  7945,   310,  1560,   688,\n",
      "         3820,   414, 29889, 17250,   368, 29892, 28134, 10021,   287, 29892,\n",
      "          541,   278,  7945, 17654, 28495,   408,   896,  1033,  9850,  1728,\n",
      "          263, 23381, 29889,  7311,   310, 28495, 29892,  7945, 21142,  8872,\n",
      "         2760,   278, 25615,   362,   664, 29889,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2], device='cuda:0')\n",
      "Batch 1: Input shape torch.Size([32, 511]), Target shape torch.Size([32, 511])\n",
      "tensor([    1,   835,  3148,  1001, 29901, 25538,   592,   263, 15837,   310,\n",
      "          920,   278,  7537,  2200, 22193, 11412,  1736,    13,    13,  2277,\n",
      "        29937,   673, 29901,   450,  7537,  2200, 22193, 11412,   338,  1304,\n",
      "          491,  2894, 12903,   304,  5706,  5864, 29889,   910,   338, 14363,\n",
      "         1549,   263,  3652,   310, 22233,   337,  7387,   393,  6403,   297,\n",
      "         1380,  2878,   898,  2849,   304,  6507,  6087,  5864,  1549,   278,\n",
      "        19100,   333,   362,   310,  1274,   300,  2904, 29899,  7967, 29909,\n",
      "        10723,   515,   285,  1446, 29892,  3279,  1144, 29892,   322,  1559,\n",
      "          833, 29882,  2941, 29878,  1078, 29889,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2], device='cuda:0') tensor([  835,  3148,  1001, 29901, 25538,   592,   263, 15837,   310,   920,\n",
      "          278,  7537,  2200, 22193, 11412,  1736,    13,    13,  2277, 29937,\n",
      "          673, 29901,   450,  7537,  2200, 22193, 11412,   338,  1304,   491,\n",
      "         2894, 12903,   304,  5706,  5864, 29889,   910,   338, 14363,  1549,\n",
      "          263,  3652,   310, 22233,   337,  7387,   393,  6403,   297,  1380,\n",
      "         2878,   898,  2849,   304,  6507,  6087,  5864,  1549,   278, 19100,\n",
      "          333,   362,   310,  1274,   300,  2904, 29899,  7967, 29909, 10723,\n",
      "          515,   285,  1446, 29892,  3279,  1144, 29892,   322,  1559,   833,\n",
      "        29882,  2941, 29878,  1078, 29889,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "qa_loader = QADataLoader(\n",
    "    dataset_name=\"databricks/databricks-dolly-15k\",\n",
    "    batch_size=batch_size,\n",
    "    block_size=block_size\n",
    ")\n",
    "\n",
    "# Test getting a batch (old style)\n",
    "x, y = qa_loader.get_batch('train')\n",
    "print(f\"Batch shapes - Input: {x.shape}, Target: {y.shape}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Decode and show a sample\n",
    "sample_input = tokenizer.decode(x[0].tolist())\n",
    "print(f\"\\nSample input (truncated):\\n{sample_input[:200]}...\")\n",
    "\n",
    "# Modern training loop example\n",
    "print(\"\\n\\nModern training loop example:\")\n",
    "train_loader, val_loader = qa_loader.get_loaders()\n",
    "print(len(train_loader))\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    print(f\"Batch {i}: Input shape {inputs.shape}, Target shape {targets.shape}\")\n",
    "    print(inputs[0], targets[0])\n",
    "    \n",
    "    if i >= 1:  # Just show first 3 batches\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
